{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as TT\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number of hidden units\n",
    "nHidden = 15\n",
    "#number of inputs\n",
    "nInputs = 3\n",
    "#number of outputs\n",
    "nOutputs = 3\n",
    "\n",
    "#input (first dimension is time)\n",
    "u = TT.matrix(\"u\")\n",
    "#target (first dimension is time)\n",
    "t = TT.matrix(\"t\")\n",
    "#initial hidden state of the RNN \n",
    "h0 = TT.vector(\"h0\")\n",
    "#learning rate\n",
    "lr = TT.scalar(\"lr\")\n",
    "#recurrent weights as a shared variable\n",
    "W = theano.shared(np.random.uniform(size = (nHidden, nHidden), low = -.01, high = .01))\n",
    "#input to hidden layer weights\n",
    "W_in = theano.shared(np.random.uniform(size = (nInputs, nHidden), low = -.01, high = .01))\n",
    "#hidden to output layer weights\n",
    "W_out = theano.shared(np.random.uniform(size = (nHidden, nInputs), low = -.01, high = .01))\n",
    "\n",
    "#recurrent function (tanh) and tanh output activation function\n",
    "def step(u_t, h_tm1, W, W_in, W_out):\n",
    "    h_t = TT.nnet.sigmoid(TT.dot(u_t, W_in) + TT.dot(h_tm1, W))\n",
    "    y_t = TT.nnet.sigmoid(TT.dot(h_t, W_out))\n",
    "    return h_t, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hidden state for entire sequence and the output for the entire sequence\n",
    "[h, y], _ = theano.scan(step, \n",
    "                       sequences = u,\n",
    "                       outputs_info = [dict(initial = TT.zeros(nHidden)), None], \n",
    "                       non_sequences = [W, W_in, W_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameliachristensen/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <type 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.python2x.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    }
   ],
   "source": [
    "# 1 step of sgd\n",
    "error = ((y-t)**2).sum()\n",
    "gW, gW_in, gW_out = TT.grad(error, [W, W_in, W_out])\n",
    "#training function computs the gradients according to penalty in error...\n",
    "\n",
    "fn = theano.function([u, t, lr], \n",
    "                    [error, y], \n",
    "                    updates = {W: W - lr*gW,\n",
    "                              W_in: W_in - lr*gW_in,\n",
    "                              W_out: W_out - lr*gW_out})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#possible pairs:\n",
    "\n",
    "# (.5, 0)\n",
    "# (.8535, .1465)\n",
    "# (1, .5)\n",
    "# (.8535, .1465)\n",
    "# (.5, 1)\n",
    "# (.1465, .8535)\n",
    "# (0, .5)\n",
    "# (.1465, .1465)\n",
    "\n",
    "loc = [[.5, 0], [.8535, .1465], [1, .5], \n",
    "       [.8535, .1465], [.5, 1], [.1465, .8535],\n",
    "      [0, .5], [.1465, .1465]];\n",
    "\n",
    "\n",
    "#probability of a match\n",
    "p = .75\n",
    "numTrials = 100000\n",
    "big_data = np.zeros([1, 3])\n",
    "big_target = np.zeros([1,3])\n",
    "\n",
    "for trial in xrange(0, numTrials):\n",
    "    memory = np.zeros([1, 2])\n",
    "    num_distractors = np.random.randint(1, 4)   \n",
    "    data = np.zeros([num_distractors + 1, 3])\n",
    "    target = np.zeros([num_distractors + 1, 3])\n",
    "    \n",
    "    for i in range(num_distractors + 1):\n",
    "        if i == 0:\n",
    "            data[i][1:3] = loc[np.random.randint(0, 8)]\n",
    "            data[i][0] = 1\n",
    "            memory = data[i][1:3]\n",
    "            target[i][1:3] = memory\n",
    "\n",
    "        elif i < num_distractors:\n",
    "            while (True):\n",
    "                data[i][1:3] = loc[np.random.randint(0, 8)]\n",
    "                if any(data[i][1:3] != memory):\n",
    "                    break;\n",
    "                    \n",
    "            target[i][1:3] = memory\n",
    "            \n",
    "        elif i == num_distractors:\n",
    "            target[i][1:3] = memory\n",
    "            if np.random.uniform()  >= p:\n",
    "                while (True):\n",
    "                    data[i][1:3] = loc[np.random.randint(0, 8)]\n",
    "                    if any(data[i][1:3] != memory):\n",
    "                        break;\n",
    "            else:\n",
    "                data[i][1:3] = memory\n",
    "                target[i][0] = 1\n",
    "                            \n",
    "\n",
    "#    print \"data:\" \n",
    "#    print data\n",
    "#    print \"target:\"\n",
    "#    print target\n",
    "#    print \"memory:\"\n",
    "    \n",
    "    big_data = np.vstack((big_data, data))\n",
    "    big_target = np.vstack((big_target, target))\n",
    "\n",
    "    \n",
    "#for i in range(len(big_data)):\n",
    "#    print big_data[i], big_target[i]\n",
    "    \n",
    "init = np.random.uniform(low = 0, high = 1, size = [nHidden])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:\n",
      "15.6235794941\n",
      "cross validation error:\n",
      "3.47058266354\n",
      "training error:\n",
      "10.5076504096\n",
      "training error:\n",
      "8.36169024638\n",
      "training error:\n",
      "6.89771166464\n",
      "training error:\n",
      "5.61788929572\n",
      "training error:\n",
      "4.77279255975\n",
      "training error:\n",
      "4.14515796364\n",
      "training error:\n",
      "3.54988716179\n",
      "training error:\n",
      "2.9772595372\n",
      "training error:\n",
      "2.50627645906\n",
      "training error:\n",
      "2.15754394337\n",
      "cross validation error:\n",
      "0.352813663898\n",
      "training error:\n",
      "1.89607658195\n",
      "training error:\n",
      "1.68694816253\n",
      "training error:\n",
      "1.51021137495\n",
      "training error:\n",
      "1.35520756626\n",
      "training error:\n",
      "1.21806536014\n",
      "training error:\n",
      "1.0991721841\n",
      "training error:\n",
      "0.999033401556\n",
      "training error:\n",
      "0.916858515531\n",
      "training error:\n",
      "0.851051850258\n",
      "training error:\n",
      "0.799698018049\n",
      "cross validation error:\n",
      "0.0595032202695\n",
      "training error:\n",
      "0.760807852221\n",
      "training error:\n",
      "0.732418835358\n",
      "training error:\n",
      "0.712673639896\n",
      "training error:\n",
      "0.699926624389\n",
      "training error:\n",
      "0.692817195104\n",
      "training error:\n",
      "0.690266335085\n",
      "training error:\n",
      "0.691409839932\n",
      "training error:\n",
      "0.69550302002\n",
      "training error:\n",
      "0.701843904363\n",
      "training error:\n",
      "0.709753092289\n",
      "cross validation error:\n",
      "0.0662601330489\n",
      "training error:\n",
      "0.718587840086\n",
      "training error:\n",
      "0.72776211986\n",
      "training error:\n",
      "0.736794147729\n",
      "training error:\n",
      "0.745352715689\n",
      "training error:\n",
      "0.753251673208\n",
      "training error:\n",
      "0.760409865873\n",
      "training error:\n",
      "0.76681037486\n",
      "training error:\n",
      "0.772468404344\n",
      "training error:\n",
      "0.777408040247\n",
      "training error:\n",
      "0.781649701976\n",
      "cross validation error:\n",
      "0.0543029009952\n",
      "training error:\n",
      "0.785210709728\n",
      "training error:\n",
      "0.788117922318\n",
      "training error:\n",
      "0.790422444545\n",
      "training error:\n",
      "0.792201147365\n",
      "training error:\n",
      "0.79354165802\n",
      "training error:\n",
      "0.794523959267\n",
      "training error:\n",
      "0.795210723786\n",
      "training error:\n",
      "0.795647127151\n",
      "training error:\n",
      "0.795865199029\n",
      "training error:\n",
      "0.795888678978\n",
      "cross validation error:\n",
      "0.0452177316253\n",
      "training error:\n",
      "0.795736676517\n",
      "training error:\n",
      "0.795425886635\n",
      "training error:\n",
      "0.794971669148\n",
      "training error:\n",
      "0.794388399236\n",
      "training error:\n",
      "0.793689427223\n",
      "training error:\n",
      "0.792886880721\n",
      "training error:\n",
      "0.791991450335\n",
      "training error:\n",
      "0.791012233066\n",
      "training error:\n",
      "0.789956663715\n",
      "training error:\n",
      "0.788830538019\n",
      "cross validation error:\n",
      "0.0403757624287\n",
      "training error:\n",
      "0.787638116617\n",
      "training error:\n",
      "0.786382291844\n",
      "training error:\n",
      "0.785064797305\n",
      "training error:\n",
      "0.783686441008\n",
      "training error:\n",
      "0.782247345432\n",
      "training error:\n",
      "0.780747181151\n",
      "training error:\n",
      "0.779185383903\n",
      "training error:\n",
      "0.777561347986\n",
      "training error:\n",
      "0.775874591368\n",
      "training error:\n",
      "0.774124890073\n",
      "cross validation error:\n",
      "0.037507149268\n",
      "training error:\n",
      "0.772312381072\n",
      "training error:\n",
      "0.770437634408\n",
      "training error:\n",
      "0.768501696353\n",
      "training error:\n",
      "0.766506106332\n",
      "training error:\n",
      "0.764452890943\n",
      "training error:\n",
      "0.762344538946\n",
      "training error:\n",
      "0.760183961208\n",
      "training error:\n",
      "0.757974439807\n",
      "training error:\n",
      "0.755719570256\n",
      "training error:\n",
      "0.753423200609\n",
      "cross validation error:\n",
      "0.0352174859273\n",
      "training error:\n",
      "0.751089370674\n",
      "training error:\n",
      "0.748722254078\n",
      "training error:\n",
      "0.746326105199\n",
      "training error:\n",
      "0.743905212414\n",
      "training error:\n",
      "0.741463858436\n",
      "training error:\n",
      "0.739006287993\n",
      "training error:\n",
      "0.736536682658\n",
      "training error:\n",
      "0.734059142257\n",
      "training error:\n",
      "0.731577672027\n",
      "training error:\n",
      "0.729096174521\n",
      "cross validation error:\n",
      "0.0328693720817\n",
      "training error:\n",
      "0.726618445096\n",
      "training error:\n",
      "0.724148169853\n",
      "training error:\n",
      "0.721688924907\n",
      "training error:\n",
      "0.719244176052\n",
      "training error:\n",
      "0.716817278159\n",
      "training error:\n",
      "0.714411473992\n",
      "training error:\n",
      "0.712029892529\n",
      "training error:\n",
      "0.709675547303\n",
      "training error:\n",
      "0.70735133502\n",
      "training error:\n",
      "0.705060036186\n",
      "cross validation error:\n",
      "0.0304514910426\n",
      "training error:\n",
      "0.702804316811\n",
      "training error:\n",
      "0.700586732263\n",
      "training error:\n",
      "0.698409732312\n",
      "training error:\n",
      "0.696275666361\n",
      "training error:\n",
      "0.694186787477\n",
      "training error:\n",
      "0.69214525377\n",
      "training error:\n",
      "0.69015312593\n",
      "training error:\n",
      "0.688212360323\n",
      "training error:\n",
      "0.686324797653\n",
      "training error:\n",
      "0.684492147975\n",
      "cross validation error:\n",
      "0.0282041827622\n",
      "training error:\n",
      "0.682715973295\n",
      "training error:\n",
      "0.680997669437\n",
      "training error:\n",
      "0.679338448874\n",
      "training error:\n",
      "0.677739326105\n",
      "training error:\n",
      "0.676201106835\n",
      "training error:\n",
      "0.674724381774\n",
      "training error:\n",
      "0.673309525377\n",
      "training error:\n",
      "0.671956699568\n",
      "training error:\n",
      "0.67066586184\n",
      "training error:\n",
      "0.66943677721\n",
      "cross validation error:\n",
      "0.02640499587\n",
      "training error:\n",
      "0.668269033132\n",
      "training error:\n",
      "0.66716205651\n",
      "training error:\n",
      "0.66611513199\n",
      "training error:\n",
      "0.665127420761\n",
      "training error:\n",
      "0.664197979259\n",
      "training error:\n",
      "0.663325777243\n",
      "training error:\n",
      "0.662509714925\n",
      "training error:\n",
      "0.661748638828\n",
      "training error:\n",
      "0.661041356287\n",
      "training error:\n",
      "0.660386648504\n",
      "cross validation error:\n",
      "0.0251225625012\n",
      "training error:\n",
      "0.659783282135\n",
      "training error:\n",
      "0.659230019497\n",
      "training error:\n",
      "0.658725627424\n",
      "training error:\n",
      "0.658268884926\n",
      "training error:\n",
      "0.657858589713\n",
      "training error:\n",
      "0.65749356374\n",
      "training error:\n",
      "0.657172657852\n",
      "training error:\n",
      "0.656894755672\n",
      "training error:\n",
      "0.656658776815\n",
      "training error:\n",
      "0.656463679527\n",
      "cross validation error:\n",
      "0.0242527707845\n",
      "training error:\n",
      "0.656308462836\n",
      "training error:\n",
      "0.656192168282\n",
      "training error:\n",
      "0.656113881306\n",
      "training error:\n",
      "0.656072732333\n",
      "training error:\n",
      "0.656067897609\n",
      "training error:\n",
      "0.65609859982\n",
      "training error:\n",
      "0.656164108515\n",
      "training error:\n",
      "0.656263740355\n",
      "training error:\n",
      "0.656396859194\n",
      "training error:\n",
      "0.656562875974\n",
      "cross validation error:\n",
      "0.0236676988825\n",
      "training error:\n",
      "0.65676124843\n",
      "training error:\n",
      "0.656991480572\n",
      "training error:\n",
      "0.657253121883\n",
      "training error:\n",
      "0.657545766188\n",
      "training error:\n",
      "0.657869050073\n",
      "training error:\n",
      "0.658222650763\n",
      "training error:\n",
      "0.658606283262\n",
      "training error:\n",
      "0.65901969655\n",
      "training error:\n",
      "0.659462668525\n",
      "training error:\n",
      "0.659934999293\n",
      "cross validation error:\n",
      "0.023279462902\n",
      "training error:\n",
      "0.660436502249\n",
      "training error:\n",
      "0.660966992167\n",
      "training error:\n",
      "0.661526269268\n",
      "training error:\n",
      "0.662114097735\n",
      "training error:\n",
      "0.662730176578\n",
      "training error:\n",
      "0.663374099783\n",
      "training error:\n",
      "0.664045301356\n",
      "training error:\n",
      "0.664742978864\n",
      "training error:\n",
      "0.665465986156\n",
      "training error:\n",
      "0.666212681852\n",
      "cross validation error:\n",
      "0.0230660123519\n",
      "training error:\n",
      "0.66698071481\n",
      "training error:\n",
      "0.667766721753\n",
      "training error:\n",
      "0.668565909928\n",
      "training error:\n",
      "0.669371512386\n",
      "training error:\n",
      "0.670174178523\n",
      "training error:\n",
      "0.670961627945\n",
      "training error:\n",
      "0.671719731046\n",
      "training error:\n",
      "0.672438386938\n",
      "training error:\n",
      "0.673128447231\n",
      "training error:\n",
      "0.673848795355\n",
      "cross validation error:\n",
      "0.0232829639828\n",
      "training error:\n",
      "0.674715440604\n",
      "training error:\n",
      "0.675866159785\n",
      "training error:\n",
      "0.677411809722\n",
      "training error:\n",
      "0.679407189519\n",
      "training error:\n",
      "0.681811902194\n",
      "training error:\n",
      "0.684404676123\n",
      "training error:\n",
      "0.686755709549\n",
      "training error:\n",
      "0.688516816233\n",
      "training error:\n",
      "0.689792046576\n",
      "training error:\n",
      "0.690960986518\n",
      "cross validation error:\n",
      "0.0229488688928\n",
      "training error:\n",
      "0.692293263535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-c80b89cd6ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         t_err[i%batch_size], y_0 = fn(big_data[i:i+batch_size],\n\u001b[0;32m---> 18\u001b[0;31m                                       big_target[i:i+batch_size], lr)        \n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ameliachristensen/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ameliachristensen/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[1;32m    671\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m    672\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#set this to stop if error increases by a large amount\n",
    "\n",
    "err = []\n",
    "batch_size = 50\n",
    "\n",
    "y_0 = np.zeros([batch_size, 3])\n",
    "targ = np.zeros([batch_size, 3])\n",
    "\n",
    "#lets do minibatch\n",
    "lr = 0.005;\n",
    "\n",
    "for j in range(1000):\n",
    "    t_err  = np.zeros([1, batch_size])\n",
    "    y_temp = np.zeros([batch_size, 3])\n",
    "    \n",
    "    for i in xrange(0, 100000 - 10, batch_size):\n",
    "        t_err[i%batch_size], y_0 = fn(big_data[i:i+batch_size],\n",
    "                                      big_target[i:i+batch_size], lr)        \n",
    "        targ = big_target[i:i+batch_size]\n",
    "    \n",
    "    \n",
    "    err.append(np.average(t_err))\n",
    "    \n",
    "    print \"training error:\"\n",
    "    print err[j]\n",
    "    \n",
    "    if j % 10 == 0:\n",
    "        e, _ = fn(big_data[100000 - 10: 100000], \n",
    "                  big_target[100000 - 10: 100000], 0)\n",
    "        \n",
    "        print \"cross validation error:\"\n",
    "        print e\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   1.   0.5] [ 0.    0.96  0.56]\n",
      "[ 0.   1.   0.5] [ 0.01  0.98  0.53]\n",
      "[ 1.   1.   0.5] [ 0.98  0.98  0.49]\n",
      "[ 0.    0.15  0.15] [ 0.    0.14  0.15]\n",
      "[ 0.    0.15  0.15] [ 0.    0.15  0.15]\n",
      "[ 0.    0.85  0.15] [ 0.    0.86  0.12]\n",
      "[ 0.    0.85  0.15] [ 0.02  0.85  0.16]\n",
      "[ 0.    0.85  0.15] [ 0.    0.83  0.15]\n",
      "[ 1.    0.85  0.15] [ 0.91  0.86  0.14]\n",
      "[ 0.   1.   0.5] [ 0.    0.96  0.51]\n",
      "[ 0.   1.   0.5] [ 0.    0.98  0.49]\n",
      "[ 0.   1.   0.5] [ 0.01  0.98  0.48]\n",
      "[ 0.   1.   0.5] [ 0.    0.98  0.45]\n",
      "[ 0.    0.15  0.85] [ 0.    0.13  0.89]\n",
      "[ 0.    0.15  0.85] [ 0.    0.15  0.87]\n",
      "[ 0.    0.15  0.85] [ 0.01  0.13  0.88]\n",
      "[ 0.    0.15  0.85] [ 0.    0.15  0.87]\n",
      "[ 1.    0.15  0.85] [ 0.96  0.16  0.85]\n",
      "[ 0.    0.85  0.15] [ 0.    0.85  0.13]\n",
      "[ 0.    0.85  0.15] [ 0.01  0.85  0.13]\n",
      "[ 0.   0.5  1. ] [ 0.    0.52  0.95]\n",
      "[ 0.   0.5  1. ] [ 0.    0.49  0.97]\n",
      "[ 0.   0.5  1. ] [ 0.    0.42  0.98]\n",
      "[ 0.   0.5  1. ] [ 0.    0.43  0.99]\n",
      "[ 0.   0.5  1. ] [ 0.    0.55  0.93]\n",
      "[ 0.   0.5  1. ] [ 0.    0.52  0.97]\n",
      "[ 0.   1.   0.5] [ 0.    0.96  0.53]\n",
      "[ 1.   1.   0.5] [ 0.95  0.97  0.52]\n",
      "[ 0.   0.   0.5] [ 0.    0.05  0.48]\n",
      "[ 0.   0.   0.5] [ 0.    0.03  0.49]\n",
      "[ 0.   0.   0.5] [ 0.    0.04  0.49]\n",
      "[ 1.   0.   0.5] [ 0.98  0.03  0.51]\n",
      "[ 0.   0.5  0. ] [ 0.    0.47  0.06]\n",
      "[ 0.   0.5  0. ] [ 0.    0.5   0.02]\n",
      "[ 0.    0.15  0.85] [ 0.    0.11  0.9 ]\n",
      "[ 0.    0.15  0.85] [ 0.    0.14  0.86]\n",
      "[ 0.    0.15  0.85] [ 0.    0.14  0.87]\n",
      "[ 0.    0.15  0.85] [ 0.    0.14  0.88]\n",
      "[ 0.    0.85  0.15] [ 0.    0.86  0.14]\n",
      "[ 0.    0.85  0.15] [ 0.    0.87  0.14]\n",
      "[ 0.    0.85  0.15] [ 0.    0.84  0.16]\n",
      "[ 1.    0.85  0.15] [ 0.99  0.85  0.15]\n",
      "[ 0.   1.   0.5] [ 0.    0.96  0.5 ]\n",
      "[ 0.   1.   0.5] [ 0.    0.98  0.49]\n",
      "[ 1.   1.   0.5] [ 0.97  0.98  0.51]\n",
      "[ 0.    0.85  0.15] [ 0.    0.86  0.12]\n",
      "[ 0.    0.85  0.15] [ 0.03  0.84  0.15]\n",
      "[ 0.    0.85  0.15] [ 0.03  0.85  0.11]\n",
      "[ 0.    0.85  0.15] [ 0.01  0.89  0.1 ]\n",
      "[ 0.   0.5  1. ] [ 0.    0.51  0.95]\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print np.round(targ[i], 2),  np.round(y_0[i], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1102352d0>]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b76d810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
